{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data, inspect, split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'features.sav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c3d7f99dc009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features.sav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'features.sav'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = joblib.load(\"features.sav\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_q= joblib.load('DQ_100_features.sav')\n",
    "data_q.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LogAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    def transform(self, X):\n",
    "        logs = {}\n",
    "        for attr in [\"mean\", \"var\", \"skew\", \"kurtosis\"]:\n",
    "            attr = \"DeltaQ_100-10_\" + attr\n",
    "            if attr in X.columns:\n",
    "                logs[\"log_\" + attr] = np.log(abs(X[attr])) \n",
    "#         logs[\"log_cycle_life\"] = np.log(X[\"cycle_life\"])\n",
    "        return pd.concat([X, pd.DataFrame(logs)], axis=1)\n",
    "\n",
    "log_adder = LogAttributesAdder()\n",
    "data = log_adder.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cor=data.corr()\n",
    "plt.figure(figsize=(55,55))\n",
    "sns.heatmap(cor,annot=True)\n",
    "plt.colorbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop NaNs from \"cycle_life\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=[\"cycle_life\"], inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data.hist(bins=30,figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and testing sets. Batches 1 and 2 are split into \"training\" and \"primary testing\" data, and batch 3 is reserved for \"secondary testing\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure that we get a good representation of both batches 1 and 2 in the training set, and that we get a good range of cycle lives (to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# stratified split for batches 1 and 2 (evenly split between batch1 and \"not batch1\" (i.e. batch2))\n",
    "batch_1_2 = data[data[\"batch3\"] == 0]\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "for train_index, test_index in split.split(batch_1_2, batch_1_2[\"batch1\"]):\n",
    "    print(\"train_index=\",train_index)\n",
    "    print(\"test_index=\",test_index)\n",
    "   \n",
    "    train_set = batch_1_2.loc[train_index]\n",
    "    test_set_1 = batch_1_2.loc[test_index]\n",
    "    \n",
    "# Secondary test set from batch 3\n",
    "test_set_2 = data[data[\"batch3\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "batch_1_2[\"batch1\"].value_counts()/len(batch_1_2)\n",
    "# batch_1_2[\"batch1\"]\n",
    "# batch_1_2\n",
    "# (train_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train[\"log_cycle_life\"]\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"std_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "var_attribs = [\"log_DeltaQ_100-10_var\",\"log_DeltaQ_100-10_mean\",\"DeltaQ_100-10_min\",\"Q_end\",\"T_min_first5\",\"log_CV_a_slope\"]\n",
    "var_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, var_attribs)\n",
    "])\n",
    "\n",
    "discharge_attribs = [\"log_DeltaQ_100-10_var\", \"log_DeltaQ_100-10_mean\",  \"log_DeltaQ_100-10_kurtosis\",  \"log_DeltaQ_100-10_skew\"]\n",
    "discharge_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, discharge_attribs)\n",
    "])\n",
    "\n",
    "train_prepared = var_pipeline.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prepared[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define error metric by taking exponential then RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(labels, predictions):\n",
    "    mse = mean_squared_error(np.exp(labels), np.exp(predictions))\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train_prepared, train_labels)\n",
    "# print(lin_reg.score(train_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy_reg = DummyRegressor()\n",
    "dummy_reg.fit(train_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function for quickly assessing different models. We are data snooping but they do it too in the paper..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def print_error_and_plot(pipeline, model, train, test_1, test_2):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([0,2600],[0,2600], \"k-\")\n",
    "    for name, test_set, marker, color in [\n",
    "        (\"Train\", train, \"o\", \"b\"), \n",
    "        (\"Primary test\", test_1, \"s\", \"r\"), \n",
    "        (\"Secondary test\", test_2, \"^\", \"y\")\n",
    "    ]:\n",
    "        X_test = test_set\n",
    "        y_test = test_set[\"log_cycle_life\"].copy()\n",
    "\n",
    "        X_test_prepared = pipeline.transform(X_test)\n",
    "#         \n",
    "        final_predictions = model.predict(X_test_prepared)\n",
    "\n",
    "#         print(final_predictions)\n",
    "        print(\"mean_abs_err=\",mean_absolute_error(y_test, final_predictions))\n",
    "#         print(\"ms_err=\",mean_squared_error(y_test, final_predictions))\n",
    "        print(\"{}: {:.0f}\".format(name, rmse(y_test, final_predictions)))\n",
    "        ax.scatter(np.exp(y_test), np.exp(final_predictions), label=name, marker=marker, color=color)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Observed cycle life\")\n",
    "    ax.set_ylabel(\"Predicted cycle life\")\n",
    "    ax.set_xlim([0,2600])\n",
    "    ax.set_ylim([0,2600])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train\n",
    "# print(x[\"log_cycle_life\"])\n",
    "# x.drop(train.columns[[3]], axis=1, inplace=True)\n",
    "\n",
    "# y.drop(train.columns[[0]], axis=1, inplace=True)\n",
    "# print(x.info())\n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = print_error_and_plot(var_pipeline, dummy_reg, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"Mean regression\")\n",
    "plt.savefig(\"dummy_regression.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = print_error_and_plot(var_pipeline, lin_reg, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"Linear regression, variance model\")\n",
    "plt.savefig(\"regression_variance.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_attribs = [\"Q_end\"]\n",
    "cap_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, cap_attribs)\n",
    "])\n",
    "train_prepared = cap_pipeline.fit_transform(train)\n",
    "lin_reg_policy = LinearRegression()\n",
    "lin_reg_policy.fit(train_prepared, train_labels)\n",
    "ax = print_error_and_plot(cap_pipeline, lin_reg_policy, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"Linear regression, charge policy model\")\n",
    "plt.savefig(\"linear_regression_policy.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning based on policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_attribs = [\"C1\", \"C2\", \"C1_percent\"]\n",
    "policy_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, policy_attribs)\n",
    "])\n",
    "train_prepared = policy_pipeline.fit_transform(train)\n",
    "lin_reg_policy = LinearRegression()\n",
    "b=lin_reg_policy.fit(train_prepared, train_labels)\n",
    "\n",
    "ax = print_error_and_plot(policy_pipeline, lin_reg_policy, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"Linear regression, charge policy model\")\n",
    "plt.savefig(\"linear_regression_policy.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_attribs = [\"Q1*C1+Q2*C2\"]\n",
    "policy_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, policy_attribs)\n",
    "])\n",
    "train_prepared = policy_pipeline.fit_transform(train)\n",
    "lin_reg_policy = LinearRegression()\n",
    "lin_reg_policy.fit(train_prepared, train_labels)\n",
    "ax = print_error_and_plot(policy_pipeline, lin_reg_policy, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"Linear regression, charge policy model\")\n",
    "plt.savefig(\"linear_regression_policy.png\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch identification\n",
    "\n",
    "Misc: there is some correlation between batch number and internal resistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for c in [1,2,3]:\n",
    "    ix = np.where(data[\"batches\"] == c)\n",
    "    d = data.iloc[ix]\n",
    "    ax.scatter(d[\"IR_min\"], d[\"T_min\"], label=\"batch {}\".format(c))\n",
    "ax.set_xlabel(\"IR at cycle 2\")\n",
    "ax.set_ylabel(\"Minimum temperature\")\n",
    "ax.legend()\n",
    "plt.savefig(\"../figures/batch_classification.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for c in [1,2,3]:\n",
    "    ix = np.where(data[\"batches\"] == c)\n",
    "    d = data.iloc[ix]\n",
    "    ax.scatter(d[\"C1\"], d[\"C2\"], label=\"batch {}\".format(c))\n",
    "ax.set_xlabel(\"Step 1 C-rate\")\n",
    "ax.set_ylabel(\"Step 2 C-rate\")\n",
    "ax.legend()\n",
    "plt.savefig(\"batch_classification_chargingrates.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor as DTR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_DT = DTR()\n",
    "op_DT = clf_DT.fit(train_prepared, train_labels)\n",
    "# op.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = print_error_and_plot(var_pipeline, op_DT, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"decision tree, variance model\")\n",
    "plt.savefig(\"DT_variance.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "clf_RF = RFR()\n",
    "op_RF = clf_RF.fit(train_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = print_error_and_plot(var_pipeline, op_RF, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"decision tree, variance model\")\n",
    "plt.savefig(\"DT_variance.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "pprint('Parameters currently in use:\\n')\n",
    "# pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomizedSearchCV for regularization and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 150)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 30, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [.1,.2,.3, .4, .5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 3]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "# pprint(random_grid)\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = clf_RF, param_distributions = random_grid, n_iter = 150, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GridsearchCv for hyperparameter tuning and regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = print_error_and_plot(var_pipeline, rf_random, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"decision tree, variance model\")\n",
    "plt.savefig(\"DT_variance.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha=0.005)\n",
    "reg.fit(train_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = print_error_and_plot(var_pipeline, reg, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"decision tree, variance model\")\n",
    "plt.savefig(\"DT_variance.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    (\"std_scaler\", StandardScaler())\n",
    "])\n",
    "# var_attribs = [\"DeltaQ_100-10_min\"]\n",
    "var_attribs = [\"log_DeltaQ_100-10_var\"]\n",
    "var_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, var_attribs)\n",
    "])\n",
    "discharge_attribs = [\"log_DeltaQ_100-10_var\", \"log_DeltaQ_100-10_mean\",  \"log_DeltaQ_100-10_kurtosis\",  \"log_DeltaQ_100-10_skew\"]\n",
    "discharge_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, discharge_attribs)\n",
    "])\n",
    "\n",
    "train_prepared = var_pipeline.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2%-3.6%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu, input_shape=[1]),\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "#     keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.RMSprop(0.0099)\n",
    "from keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "    \n",
    "model.compile(optimizer = \"rmsprop\", loss = root_mean_squared_error, \n",
    "              metrics =[\"accuracy\"])\n",
    "model.fit(train_prepared, train_labels,epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_print_error_and_plot(pipeline, model, train, test_1, test_2):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([0,2600],[0,2600], \"k-\")\n",
    "    for name, test_set, marker, color in [\n",
    "        (\"Train\", train, \"o\", \"b\"), \n",
    "        (\"Primary test\", test_1, \"s\", \"r\"), \n",
    "        (\"Secondary test\", test_2, \"^\", \"y\")\n",
    "    ]:\n",
    "        X_test = test_set\n",
    "        y_test = test_set[\"log_cycle_life\"].copy()\n",
    "\n",
    "        X_test_prepared = pipeline.transform(X_test)\n",
    "#         \n",
    "        final_predictions = model.predict(X_test_prepared)\n",
    "        final_predictions =np.append(final_predictions, final_predictions, axis=1)\n",
    "#         print(final_predictions)\n",
    "        print(\"mean_abs_err=\",mean_absolute_error(y_test, final_predictions))\n",
    "#         print(\"ms_err=\",mean_squared_error(y_test, final_predictions))\n",
    "        print(\"{}: {:.0f}\".format(name, rmse(y_test, final_predictions)))\n",
    "        ax.scatter(np.exp(y_test), np.exp(final_predictions), label=name, marker=marker, color=color)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Observed cycle life\")\n",
    "    ax.set_ylabel(\"Predicted cycle life\")\n",
    "    ax.set_xlim([0,2600])\n",
    "    ax.set_ylim([0,2600])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = NN_print_error_and_plot(var_pipeline, model, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"Neural Network, variance model\")\n",
    "plt.savefig(\"NN_variance.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"log_delta_\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Approach\n",
    "Error=3% feature:\"log_DeltaQ_100-10_var\",\"DeltaQ_100-10_min\"\n",
    "Error=3.15% By adding two features: \"log_DeltaQ_100-10_var\",\"Q_end\"\n",
    "Error=3.3% When only 1 feature: log_DeltaQ_100-10_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def print_error_and_plot(pipeline, model, train, test_1, test_2):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([0,2600],[0,2600], \"k-\")\n",
    "    for name, test_set, marker, color in [\n",
    "        (\"Train\", train, \"o\", \"b\"), \n",
    "        (\"Primary test\", test_1, \"s\", \"r\"), \n",
    "        (\"Secondary test\", test_2, \"^\", \"y\")\n",
    "    ]:\n",
    "        X_test = test_set\n",
    "        y_test = test_set[\"log_cycle_life\"].copy()\n",
    "\n",
    "        X_test_prepared = pipeline.transform(X_test)\n",
    "#         \n",
    "        final_predictions = model.predict(X_test_prepared)\n",
    "       \n",
    "        print(\"mean_abs_err=\",mean_absolute_error(y_test, final_predictions))\n",
    "        print(\"{}: {:.0f}\".format(name, rmse(y_test, final_predictions)))\n",
    "        ax.scatter(np.exp(y_test), np.exp(final_predictions), label=name, marker=marker, color=color)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Observed cycle life\")\n",
    "    ax.set_ylabel(\"Predicted cycle life\")\n",
    "    ax.set_xlim([0,2600])\n",
    "    ax.set_ylim([0,2600])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cor=data.corr()\n",
    "plt.figure(figsize=(55,55))\n",
    "sns.heatmap(cor,annot=True)\n",
    "plt.colorbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    (\"std_scaler\", StandardScaler())\n",
    "])\n",
    "# var_attribs = [\"DeltaQ_100-10_min\"]\n",
    "var_attribs = [\"log_DeltaQ_100-10_var\",\"log_DeltaQ_100-10_mean\",\"DeltaQ_100-10_min\",\"Q_end\",\"T_min_first5\"]\n",
    "var_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, var_attribs)\n",
    "])\n",
    "discharge_attribs = [\"log_DeltaQ_100-10_var\", \"log_DeltaQ_100-10_mean\",  \"log_DeltaQ_100-10_kurtosis\",  \"log_DeltaQ_100-10_skew\"]\n",
    "discharge_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, discharge_attribs)\n",
    "])\n",
    "\n",
    "train_prepared = var_pipeline.fit_transform(train)\n",
    "# train_prepared = discharge_pipeline.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "kernel = DotProduct() + WhiteKernel()\n",
    "model = GaussianProcessRegressor(kernel=kernel,\n",
    "         random_state=0).fit(train_prepared, train_labels)\n",
    "\n",
    "ax =print_error_and_plot(var_pipeline, model, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"Bayesian, variance model\")\n",
    "plt.savefig(\"Bayesian.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr=SVR()\n",
    "svr= SVR(kernel=\"poly\", degree=2, coef0=5, gamma=1, C=5, epsilon=.11)\n",
    "svr.fit(train_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried to apply Randomized Search to tune hyperparameter but unable to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "kernel=\"linear\"\n",
    "# Number of features to consider at every split\n",
    "# Maximum number of levels in tree\n",
    "gamma= [.5,1,1.5]\n",
    "\n",
    "# Minimum number o samples required to split a node\n",
    "C=[2,5,7]\n",
    "# Minimum number of samples required at each leaf node\n",
    "epsilon=[1,2,3\n",
    "        ]\n",
    "# Method of selecting samples for training each tree\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'kernel': kernel,\n",
    "               \n",
    "               'gamma': gamma,\n",
    "               'C': C,\n",
    "               'epsilon': epsilon,\n",
    "               }\n",
    "# pprint(random_grid)\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "svr_random = RandomizedSearchCV(estimator = svr, param_distributions = random_grid, n_iter = 100, cv = 5)\n",
    "# Fit the random search model\n",
    "svr_random.fit(train_prepared, train_labels)\n",
    "svr_random.best_params_\n",
    "ax =print_error_and_plot(var_pipeline, clf, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"SVM, variance model\")\n",
    "plt.savefig(\"SVM.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRid Seach to tune the SVM hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "parameters = {'kernel': ('linear', 'rbf','poly'), 'C':[.5,.7,.9 ],'gamma': [1e-7, 5e-7,5e-8],'epsilon':[0.15,0.2,0.25,0.3]}\n",
    "svr = SVR()\n",
    "clf = GridSearchCV(svr, parameters)\n",
    "clf.fit(train_prepared, train_labels)\n",
    "pprint(clf.best_params_)\n",
    "ax =print_error_and_plot(var_pipeline, clf, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"SVM, variance model\")\n",
    "plt.savefig(\"SVM.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance Vector Machine (RVM)\n",
    "Use the below command to install RVR\n",
    "pip install https://github.com/JamesRitchie/scikit-rvm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrvm import RVR\n",
    "clf = RVR(kernel='linear')\n",
    "clf.fit(train_prepared, train_labels)\n",
    "RVR(alpha=1e-06, beta=1e-06, beta_fixed=False, bias_used=True, coef0=0.0,\n",
    "coef1=None, degree=3, kernel='linear', n_iter=3000,\n",
    "threshold_alpha=1000000000.0, tol=0.001, verbose=False)\n",
    "ax =print_error_and_plot(var_pipeline, clf, train, test_set_1, test_set_2)\n",
    "ax.set_title(\"RVM\")\n",
    "plt.savefig(\"RVM.png\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
